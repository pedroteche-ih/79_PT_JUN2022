{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df850926",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Análise-de-Componentes-Principais-(PCA)\" data-toc-modified-id=\"Análise-de-Componentes-Principais-(PCA)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Análise de Componentes Principais (PCA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simulação-I\" data-toc-modified-id=\"Simulação-I-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Simulação I</a></span></li><li><span><a href=\"#Simulação-II\" data-toc-modified-id=\"Simulação-II-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Simulação II</a></span></li><li><span><a href=\"#Aplicação-I:-Diamantes\" data-toc-modified-id=\"Aplicação-I:-Diamantes-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Aplicação I: Diamantes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Etapa-1:-Carregar-e-transformar-os-Dados\" data-toc-modified-id=\"Etapa-1:-Carregar-e-transformar-os-Dados-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Etapa 1: Carregar e transformar os Dados</a></span></li><li><span><a href=\"#Etapa-2:-Normalizar-os-dados\" data-toc-modified-id=\"Etapa-2:-Normalizar-os-dados-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Etapa 2: Normalizar os dados</a></span></li><li><span><a href=\"#Etapa-3:-Utilizando-PCA\" data-toc-modified-id=\"Etapa-3:-Utilizando-PCA-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Etapa 3: Utilizando PCA</a></span></li><li><span><a href=\"#Etapa-4:-Análise\" data-toc-modified-id=\"Etapa-4:-Análise-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Etapa 4: Análise</a></span></li><li><span><a href=\"#Análise-de-Loadings\" data-toc-modified-id=\"Análise-de-Loadings-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Análise de Loadings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualização-de-dados\" data-toc-modified-id=\"Visualização-de-dados-1.3.5.1\"><span class=\"toc-item-num\">1.3.5.1&nbsp;&nbsp;</span>Visualização de dados</a></span></li></ul></li></ul></li><li><span><a href=\"#Aplicação-2:-NLP\" data-toc-modified-id=\"Aplicação-2:-NLP-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Aplicação 2: NLP</a></span><ul class=\"toc-item\"><li><span><a href=\"#Carregando-dados\" data-toc-modified-id=\"Carregando-dados-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Carregando dados</a></span></li><li><span><a href=\"#Conhecendo-o-CountVectorizer\" data-toc-modified-id=\"Conhecendo-o-CountVectorizer-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Conhecendo o CountVectorizer</a></span></li><li><span><a href=\"#Reduzindo-o-vocabulário\" data-toc-modified-id=\"Reduzindo-o-vocabulário-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>Reduzindo o vocabulário</a></span></li><li><span><a href=\"#Modelando\" data-toc-modified-id=\"Modelando-1.4.4\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>Modelando</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec04ab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:38:59.947728Z",
     "start_time": "2022-03-17T22:38:58.705927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2887978",
   "metadata": {},
   "source": [
    "# Análise de Componentes Principais (PCA)\n",
    "\n",
    "PCA é um, fundamentalmente, um **algoritmo de redução dimensional**: utilizando PCA, podemos transformar as variáveis de nossa dataset de forma a reduzir o número total de variáveis. O algoritmo utiliza a **covariância entre variáveis** para estimar *novas variáveis* com duas propriedades:\n",
    "\n",
    "1. As novas variáveis são combinações lineares das variáveis originais;\n",
    "1. Elas são ortogonais entre si, ou seja, tem correlação 0!\n",
    "\n",
    "Se temos um conjunto de dados com muitas variáveis correlatas entre si podemos utilizar PCA para *reduzir a redundância de informação*: as novas variáveis calculadas pelo algoritmo não serão correlatas! O algoritmo de PCA tem diversas aplicações em ML (e serve como *protótipo* para os outros algoritmos de *redução dimensional* que aprenderermos ao longo do bootcamp):\n",
    "\n",
    "1. Eliminar correlação entre variáveis de entrada em um modelo de regressão;\n",
    "1. Facilitar a visualização de relações em datasets de alta dimensionalidade;\n",
    "1. Reduzir a redundância de variáveis e estimar o número **real** de dimensões independentes.\n",
    "\n",
    "Antes de mergulharmos nas aplicações, vamos entender e visualizar como o algoritmo funciona a partir de um conjunto de dados sintético.\n",
    "\n",
    "## Simulação I\n",
    "\n",
    "Vamos começar nossa simulação vendo como o algoritmo PCA se comporta quando aplicado sobre um dataset de 2 variáveis altamente correlatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c40bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:39:14.592871Z",
     "start_time": "2022-03-17T22:39:14.587884Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def simular_dado_mv_x(parametros_x1, parametros_x2, samples):\n",
    "    x1 = np.random.normal(loc=parametros_x1[0], scale=parametros_x1[1], size=samples)\n",
    "    x2 = x1 + np.random.normal(\n",
    "        loc=parametros_x2[0], scale=parametros_x2[1], size=samples\n",
    "    )\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"x1\": x1,\n",
    "            \"x2\": x2,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481c199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:39:15.559021Z",
     "start_time": "2022-03-17T22:39:15.544072Z"
    }
   },
   "outputs": [],
   "source": [
    "tb_simul_x = simular_dado_mv_x((0, 5), (0, 1), 100)\n",
    "tb_simul_x.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77cf3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:39:28.869705Z",
     "start_time": "2022-03-17T22:39:28.660153Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=tb_simul_x, x=\"x1\", y=\"x2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad159e2e",
   "metadata": {},
   "source": [
    "Para utilizarmos o algoritmo PCA precisamos garantir que nossos dados estão **normalizados**. Para tanto, utilizaremos o `StandardScaler` da biblioteca `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8468fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46fd85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:46:21.726776Z",
     "start_time": "2022-03-17T22:46:21.705817Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(tb_simul_x)\n",
    "X_norm = scaler.transform(tb_simul_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557d971",
   "metadata": {},
   "source": [
    "Agora que temos nossos dados normalizados, podemos aplicar o algoritmo à nossa tabela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a65099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:48:29.131789Z",
     "start_time": "2022-03-17T22:48:29.111843Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_norm)\n",
    "pca_X_norm = pca.transform(X_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef8643",
   "metadata": {},
   "source": [
    "O resultado do método `.transform` é a **transformação do dataset `X_norm`** utilizando o algoritmo PCA estimado a partir do **dataset `X_norm`** - como escolhemos 2 componentes, será uma matriz de duas colunas e com o mesmo número de pontos que `X_norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6386ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:48:35.369221Z",
     "start_time": "2022-03-17T22:48:35.348494Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_X_norm[0:5, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2cd02",
   "metadata": {},
   "source": [
    "Em varias aplicações, um array será suficiente. Mas para visualizar nossos componentes, podemos transformar este array em um `DataFrame` e junta-lo aos dados originais e normalizados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a0a95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:49:32.682830Z",
     "start_time": "2022-03-17T22:49:32.677844Z"
    }
   },
   "outputs": [],
   "source": [
    "tb_sca_x = pd.DataFrame(X_norm, columns=[\"X1_sca\", \"X2_sca\"])\n",
    "tb_pca_x = pd.DataFrame(pca_X_norm, columns=[\"PC1\", \"PC2\"])\n",
    "tb_full_x = pd.concat([tb_simul_x, tb_sca_x, tb_pca_x], axis=1)\n",
    "\n",
    "tb_full_x.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613e0f0",
   "metadata": {},
   "source": [
    "Podemos utilizar o atributo `.components` para visualizar os **loadings**: os coeficientes de combinação linear estimados para cada componente/variável:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41288159",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_.T\n",
    "components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c52e34",
   "metadata": {},
   "source": [
    "Vamos converter o resultado disso em um `DataFrame` para ficar mais claro o que estamos vendo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(components, columns=[\"PC1\", \"PC2\"], index=tb_simul_x.columns)\n",
    "loadings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d95704",
   "metadata": {},
   "source": [
    "Uma forma de entender PCA é pensar que os eixos X1 e X2 podem ser *rotacionados* e os nossos pontos de dados podem ser **projetados** sobre esses novos eixos. Para visualizar essa rotação vamos utilizar um tipo de *biplot*, que mostra como os eixos da PCA (os componentes `PC_0` e `PC_1`) estão em relação aos eixos originais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88907d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:49:35.649112Z",
     "start_time": "2022-03-17T22:49:35.501000Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(data=tb_full_x, x=\"X1_sca\", y=\"X2_sca\")\n",
    "plt.arrow(0, 0, components[0, 0], components[1, 0], color=\"red\")\n",
    "plt.text(\n",
    "    components[0, 0] * 1.25,\n",
    "    components[1, 0] * 1.25,\n",
    "    \"PC_0\",\n",
    "    color=\"black\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=\"large\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.arrow(0, 0, components[0, 1], components[1, 1], color=\"red\")\n",
    "plt.text(\n",
    "    components[0, 1] * 1.25,\n",
    "    components[1, 1] * 1.25,\n",
    "    \"PC_1\",\n",
    "    color=\"black\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=\"large\",\n",
    "    fontweight=\"bold\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e778b1",
   "metadata": {},
   "source": [
    "## Simulação II\n",
    "\n",
    "The PCA algorithm will try to find the directions in which the most information is contained. By information, we always mean to say - variance.\n",
    "\n",
    "Rather than attempting to *predict* the y values from the x values, the unsupervised learning problem attempts to learn about the *relationship* between your features (your `X`).\n",
    "\n",
    "In principal component analysis, this relationship is quantified by finding a list of the *principal axes* in the data, and using those axes to describe the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b54b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simular_dado_mv(parametros_x1, parametros_x2, desvpad_E, samples):\n",
    "    x1 = np.random.normal(loc=parametros_x1[0], scale=parametros_x1[1], size=samples)\n",
    "    x2 = x1 + np.random.normal(\n",
    "        loc=parametros_x2[0], scale=parametros_x2[1], size=samples\n",
    "    )\n",
    "    E = np.random.normal(loc=0, scale=desvpad_E, size=samples)\n",
    "    y = parametros_x1[2] * x1 + parametros_x2[2] * x2 + E\n",
    "    x3 = (desvpad_E / 2) * y + np.random.normal(\n",
    "        loc=0, scale=desvpad_E * 3, size=samples\n",
    "    )\n",
    "    return pd.DataFrame({\"y\": y, \"X1\": x1, \"X2\": x2, \"X3\": x3})\n",
    "\n",
    "\n",
    "tb_sim = simular_dado_mv((0, 10, 2), (0, 5, -2), 4, 100)\n",
    "tb_sim.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(tb_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f7f2e",
   "metadata": {},
   "source": [
    "Vamos criar duas regressões para estimar a relação entre as variáveis X e y de nosso dataset:\n",
    "\n",
    "1. A variável `X3` parece ser a mais correlata com nossa variável resposta, então vamos utilizar uma regressão simples com `X3` como variável de entrada e `y` como variável resposta;\n",
    "1. Vamos construir uma segunda regressão utilizando `X1` e `X2` como variáveis de entrada e `y` como variável resposta.\n",
    "\n",
    "Para mensurar a capacidade preditiva de cada uma desses features, vamos utilizar o RMSE destas duas regressões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfabf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tb_sim[[\"X3\"]]\n",
    "y = tb_sim[\"y\"]\n",
    "fit_x3 = LinearRegression()\n",
    "fit_x3.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306adecf",
   "metadata": {},
   "source": [
    "Vamos calcular o erro desta primeira regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_x3 = np.sqrt(mean_squared_error(y, fit_x3.predict(X)))\n",
    "print(np.round(rmse_x3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a931d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tb_sim[[\"X1\", \"X2\"]]\n",
    "y = tb_sim[\"y\"]\n",
    "fit_x1x2 = LinearRegression()\n",
    "fit_x1x2.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5adbc",
   "metadata": {},
   "source": [
    "Agora, vamos calcular o erro de nossa segunda regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c48fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_x1x2 = np.sqrt(mean_squared_error(y, fit_x1x2.predict(X)))\n",
    "print(np.round(rmse_x1x2, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75e7d0",
   "metadata": {},
   "source": [
    "Embora a correlação entre `X3` e `y` seja maior que as correlações de `X1` e `X2`, a segunda regressão apresentou um erro bem inferior à primeira. O que está acontencendo? Vamos analisar os coeficientes de nossa segunda regressão para entender melhor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78307212",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_x1x2.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2104df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_sim.corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410d607",
   "metadata": {},
   "source": [
    "Nas duas tabelas acima podemos ver duas coisas:\n",
    "\n",
    "1. `X1` e `y` são diretamente proporcionais enquanto `X2` e `y` são inversamente proporcionais;\n",
    "1. `X1` e `X2` são positivamente correlatos.\n",
    "\n",
    "Quando visualizamos as relações entre essas três variáveis a correlação entre `X1` e `X2` *oculta* a relação destas com `y`! Na prática, esse tipo de redundância pode nos levar à uma avaliação errônea das variáveis importantes de nosso dataset quando utilizamos comparações um à um de nossas variáveis!\n",
    "\n",
    "Vamos utilizar PCA para remover a estrutura de correlação de nossas variáveis de entrada antes de alimentarmos ao nosso modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee41b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = StandardScaler()\n",
    "norm.fit(tb_sim[[\"X1\", \"X2\"]])\n",
    "\n",
    "norm_x1x2 = norm.transform(tb_sim[[\"X1\", \"X2\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_t = PCA(n_components=2)\n",
    "pca_t.fit(norm_x1x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pd.DataFrame(pca_t.transform(norm_x1x2), columns=[\"PC0\", \"PC1\"])\n",
    "tb_sim_pca = pd.concat([tb_sim, X_pca], axis=1)\n",
    "tb_sim_pca.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67628c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(tb_sim_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(pca_t.components_.T, columns=[\"PC1\", \"PC2\"], index=[\"X1\", \"X2\"])\n",
    "loadings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351b02d",
   "metadata": {},
   "source": [
    "## Aplicação I: Diamantes\n",
    "\n",
    "Até agora vimos como PCA pode ser utilizada para transformar nossas variáveis em novas variáveis, busca simplificar a estrutura de correlação entre nossas variáveis X. \n",
    "\n",
    "Na segunda simulação, vimos como utilizando PCA conseguimos *desvendar* certas relações que estão escondidas pela correlação entre variáveis de entrada.\n",
    "\n",
    "Agora, vamos utilizar *PCA para simplificar* um conjunto de dados com o qual trabalhamos: **diamantes**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8adaefb",
   "metadata": {},
   "source": [
    "### Etapa 1: Carregar e transformar os Dados\n",
    "\n",
    "Vamos utilizar uma função pré-definida (que vimos na apresentação das soluções do projeto) para ler e transformar nossos dados, criando alguns features a partir das sugestões do *notebook* da solução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e77ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:55:52.307406Z",
     "start_time": "2022-03-17T22:55:52.285476Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def etl_diamonds(diamonds):\n",
    "    print(\"Linhas na base original:\", str(len(diamonds)))\n",
    "    x_0 = diamonds[\"x\"] == 0\n",
    "    y_0 = diamonds[\"y\"] == 0\n",
    "    z_0 = diamonds[\"z\"] == 0\n",
    "    dim_0 = x_0 | y_0 | z_0\n",
    "    diamonds_c = diamonds[~dim_0].copy()\n",
    "    dict_clarity = {\n",
    "        \"I1\": 0,\n",
    "        \"SI2\": 1,\n",
    "        \"SI1\": 2,\n",
    "        \"VS2\": 3,\n",
    "        \"VS1\": 4,\n",
    "        \"VVS2\": 5,\n",
    "        \"VVS1\": 6,\n",
    "        \"IF\": 7,\n",
    "    }\n",
    "    diamonds_c[\"clarity_num\"] = diamonds_c[\"clarity\"].map(dict_clarity)\n",
    "    dict_color = {\n",
    "        \"D\": 0,\n",
    "        \"E\": 1,\n",
    "        \"F\": 2,\n",
    "        \"G\": 3,\n",
    "        \"H\": 4,\n",
    "        \"I\": 5,\n",
    "        \"J\": 6,\n",
    "    }\n",
    "    diamonds_c[\"color_num\"] = diamonds_c[\"color\"].map(dict_color)\n",
    "    dict_cut = {\n",
    "        \"Fair\": 0,\n",
    "        \"Good\": 1,\n",
    "        \"Very Good\": 2,\n",
    "        \"Premium\": 3,\n",
    "        \"Ideal\": 4,\n",
    "    }\n",
    "    diamonds_c[\"cut_num\"] = diamonds_c[\"cut\"].map(dict_cut)\n",
    "    diamonds_c[\"volume\"] = diamonds_c[\"x\"] * diamonds_c[\"y\"] * diamonds_c[\"z\"]\n",
    "    diamonds_c[\"density\"] = diamonds_c[\"carat\"] / diamonds_c[\"volume\"]\n",
    "    density_inf = np.quantile(diamonds_c[\"density\"], 0.01)\n",
    "    density_sup = np.quantile(diamonds_c[\"density\"], 0.99)\n",
    "    density_in = (density_inf < diamonds_c[\"density\"]) & (\n",
    "        diamonds_c[\"density\"] < density_sup\n",
    "    )\n",
    "    diamonds_c = diamonds_c[density_in].copy()\n",
    "\n",
    "    diamonds_c[\"log_carat\"] = np.log(diamonds_c[\"carat\"])\n",
    "    diamonds_c[\"log_price\"] = np.log(diamonds_c[\"price\"])\n",
    "    diamonds_c = diamonds_c.drop(\n",
    "        [\"cut\", \"color\", \"price\", \"clarity\", \"density\", \"carat\", \"clarity\", \"volume\", \"density\"], axis=1\n",
    "    ).reset_index()\n",
    "    print(\"Linhas na etapa atual:\", str(len(diamonds_c)))\n",
    "    return diamonds_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ef2c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:56:14.530232Z",
     "start_time": "2022-03-17T22:56:14.414539Z"
    }
   },
   "outputs": [],
   "source": [
    "tb_diamonds = pd.read_csv(\"data/tb_diamantes.csv\")\n",
    "tb_diamonds_c = etl_diamonds(tb_diamonds)\n",
    "tb_diamonds_c.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_var = [\n",
    "    \"depth\",\n",
    "    \"table\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    \"clarity_num\",\n",
    "    \"color_num\",\n",
    "    \"cut_num\",\n",
    "    \"log_carat\",\n",
    "]\n",
    "y_var = \"log_price\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595999b8",
   "metadata": {},
   "source": [
    "A função acima converteu todas as variáveis categóricas originais em variáveis ordinais. Nosso dataset após as transformações e limpezas contém apenas variáveis numéricas e sem NAs\n",
    "\n",
    "### Etapa 2: Normalizar os dados\n",
    "\n",
    "Como vamos utilizar PCA, precisamos *normalizar* nossos dados. Vamos construir um `StandardScaler` para as nossas variáveis de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728e28d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:57:08.753491Z",
     "start_time": "2022-03-17T22:57:08.707325Z"
    }
   },
   "outputs": [],
   "source": [
    "norm = StandardScaler()\n",
    "norm.fit(tb_diamonds_c[X_var])\n",
    "\n",
    "tb_diamonds_norm = pd.DataFrame(norm.transform(tb_diamonds_c[X_var]), columns=X_var)\n",
    "tb_diamonds_norm['log_price'] = tb_diamonds_c['log_price']\n",
    "tb_diamonds_norm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522bbe3",
   "metadata": {},
   "source": [
    "Vamos visualizar nossa matriz de correlação para entender se podemos ter problemas de colinearidade entre as variáveis de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b92890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T22:59:20.011420Z",
     "start_time": "2022-03-17T22:58:54.962512Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(tb_diamonds_norm.corr(), vmin=-1, vmax=1, center=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa1806",
   "metadata": {},
   "source": [
    "### Etapa 3: Utilizando PCA\n",
    "\n",
    "Com nosso dataset limpo e normalizado, podemos utilizar PCA para **estimar a quantidade real de informação que temos**: *features* altamente correlatas **são redundantes** e **ocultam relações com nossa variável resposta**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bab86b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:21:47.379861Z",
     "start_time": "2022-03-17T23:21:47.335978Z"
    }
   },
   "outputs": [],
   "source": [
    "X = tb_diamonds_norm.drop(\"log_price\", axis=1)\n",
    "pca_t = PCA()\n",
    "pca_t.fit(X)\n",
    "pca_X_norm = pca_t.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a07c2",
   "metadata": {},
   "source": [
    "Além de estimar os componentes, o algoritmo de PCA calcula **a % de informação sobre o dataset original está contida em cada componente**. Podemos acessar essa informação através do atributo `.explained_variance_ratio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.round(x, 2) for x in pca_t.explained_variance_ratio_]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e017f90",
   "metadata": {},
   "source": [
    "Como podemos ver acima, o primeiro componente representa, sozinho, **53% da informação contida nas 9 variáveis originais!** Uma forma comum de avaliar o número de componentes que devemos utilizar em nossa análise é através do *scree plot*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c3f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:22:18.066175Z",
     "start_time": "2022-03-17T23:22:17.933526Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pca_t.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1633d8",
   "metadata": {},
   "source": [
    "O *scree plot* acima nos mostra que a partir do segundo cada componente adicional representa muito pouca informação. Vamos utilizar esta leitura para inicializar nosso PCA com apenas 2 componentes através do argumento `n_components`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33149207",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tb_diamonds_norm.drop(\"log_price\", axis=1)\n",
    "pca_t = PCA(n_components=2)\n",
    "pca_t.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc207f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:19:09.836397Z",
     "start_time": "2022-03-17T23:19:09.823433Z"
    }
   },
   "outputs": [],
   "source": [
    "tb_pca_diam = pd.DataFrame(\n",
    "    pca_t.transform(X), columns=[\"PC_\" + str(i) for i in range(pca_t.n_components_)]\n",
    ")\n",
    "\n",
    "tb_pca_diam[\"log_price\"] = tb_diamonds_norm[\"log_price\"]\n",
    "tb_pca_diam.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f0ae7",
   "metadata": {},
   "source": [
    "### Etapa 4: Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ec3a5",
   "metadata": {},
   "source": [
    "### Análise de Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051dce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(pca_t.components_.T, columns=[\"PC_0\", \"PC_1\"], index=X.columns)\n",
    "loadings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "p1 = sns.scatterplot(data=loadings, x=\"PC_0\", y=\"PC_1\")\n",
    "for line in range(0, loadings.shape[0]):\n",
    "    p1.text(\n",
    "        loadings.iloc[line, 0] + 0.05,\n",
    "        loadings.iloc[line, 1],\n",
    "        loadings.index[line],\n",
    "        horizontalalignment=\"left\",\n",
    "        size=\"medium\",\n",
    "        color=\"black\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271a1f6",
   "metadata": {},
   "source": [
    "#### Visualização de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842f45b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:19:14.033990Z",
     "start_time": "2022-03-17T23:19:12.707908Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=tb_pca_diam, x=\"PC_0\", y=\"log_price\", hue=\"PC_1\", palette = 'Spectral', s = 5, alpha = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c9c05",
   "metadata": {},
   "source": [
    "Com a análise acima podemos concluir que as informações são altamente redundantes - apenas o componente 0 apresenta correlação direta com a variável resposta. Sabendo isto, poderíamos ou buscar mais informações ou voltar ao processo de exploração de dados para construir novos *features* (discretizando variáveis continuas por exemplo) ou explorar nossos *features* categóricos.\n",
    "\n",
    "## Aplicação 2: NLP\n",
    "\n",
    "Além da importância no processo de exploração, PCA pode ser utilizado para reduzir informações antes de utiliza-las em um modelo. Uma forma comum desta utilização acontece quando temos mais features do que pontos - algo comum nas tarefas de NLP (*natural language processing*).\n",
    "\n",
    "Vamos utilizar PCA para construir uma regressão capaz de prever a quantidade de gordura em uma tabela de ingredientes a partir da **descrição deste ingrediente**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbbab4",
   "metadata": {},
   "source": [
    "### Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a902c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:49:28.812513Z",
     "start_time": "2022-03-17T23:49:28.784563Z"
    }
   },
   "outputs": [],
   "source": [
    "food = pd.read_csv(\"data/Food Composition.csv\")\n",
    "food = food[food['Fat Factor'] > 0].copy()\n",
    "food.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dee3b6",
   "metadata": {},
   "source": [
    "Vamos criar um campo novo a partir da concatenação das variáveis `Food Name` e `Food Description`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76a956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:50:45.503214Z",
     "start_time": "2022-03-17T23:50:45.489250Z"
    }
   },
   "outputs": [],
   "source": [
    "food[\"text\"] = food[\"Food Name\"] + \" \" + food[\"Food Description\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728c88d",
   "metadata": {},
   "source": [
    "### Conhecendo o CountVectorizer\n",
    "\n",
    "Para trabalharmos com texto (texto livre, não categorias) precisamos buscar alguma estratégia para transformar este texto em variáveis numéricas. Uma forma simples é através do modelo *Bag-of-Word*: vamos criar uma coluna numérica para cada palavra que aparece em nossas descrições. Essa coluna conterá o número de ocorrências de sua palavra em cada um de nossos textos.\n",
    "\n",
    "Para construir essa matriz podemos utilizar o `CountVectorizer` da `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc641e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78de3a",
   "metadata": {},
   "source": [
    "O `CountVectorizer` se comporta como o `StandardScaler`: ele possui um método `.fit()` e um método `.transform()`. Vamos utilizar esses dois métodos para encontrar nossa matriz de *bag-of-words*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd2e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T00:07:19.004550Z",
     "start_time": "2022-03-18T00:07:18.952662Z"
    }
   },
   "outputs": [],
   "source": [
    "ck = CountVectorizer(stop_words=\"english\")\n",
    "ck_fit = ck.fit(food[\"text\"])\n",
    "X = ck_fit.transform(food[\"text\"]).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9cef5b",
   "metadata": {},
   "source": [
    "Podemos transformar X em um DataFrame, mas como não vamos visualiza-lo diretamente (**2094** colunas!) esta etapa não é necessária."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5281ed0d",
   "metadata": {},
   "source": [
    "### Reduzindo o vocabulário\n",
    "\n",
    "O número de colunas é muito grande e provavelmente cheio de colinearidades. Como o dataset é grande demais para visualizarmos, precisaremos utilizar uma redução dimensional antes de alimentar estes dados ao nosso modelo.\n",
    "\n",
    "Vamos utilizar nosso *scree plot* para escolher o número de componentes em nosso PCA:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabf53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = StandardScaler()\n",
    "norm.fit(X)\n",
    "\n",
    "X_sca = norm.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_t = PCA()\n",
    "pca_t.fit(X_sca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f36158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,7))\n",
    "ax[0].plot(pca_t.explained_variance_ratio_)\n",
    "ax[1].plot(np.cumsum(pca_t.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42039b",
   "metadata": {},
   "source": [
    "Pelos gráficos acima podemos ver que a partir de **800 componentes** não temos quase nenhuma informação adicional. Entretanto, temos apenas **1045** linhas de dados - um modelo com 800 variáveis ainda tem variáveis demais!\n",
    "\n",
    "Olhando para o *scree plot** podemos ver que a quantidade de informação adicional cai radicalmente após cerca de 190 componentes. Vamos testar um modelo com este número e avaliar o erro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b8695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T00:07:33.871648Z",
     "start_time": "2022-03-18T00:07:33.616285Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_t = PCA(n_components=190)\n",
    "pca_t.fit(X_sca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41566107",
   "metadata": {},
   "source": [
    "Os loadings de uma redução dimensional do modelo *bag-of-words* representam um modelo **muito comum** em NLP: **o modelo de tópicos**. Vamos ordernar nossa tabela de loadings por alguns de nossos componentes para visualizar o que é um modelo de tópicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa3084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:58:02.117984Z",
     "start_time": "2022-03-17T23:58:02.088066Z"
    }
   },
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(pca_t.components_.T, index=ck.get_feature_names_out())\n",
    "loadings.sort_values(1, ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfceb28",
   "metadata": {},
   "source": [
    "### Modelando\n",
    "\n",
    "Com nossas novas variáveis podemos modelar a variável `Fat Factor` utilizando nossos componentes, criados a partir das descrições dos ingredientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0965c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:59:45.823192Z",
     "start_time": "2022-03-17T23:59:45.772330Z"
    }
   },
   "outputs": [],
   "source": [
    "X_pca = pca_t.transform(X_sca)\n",
    "y = food[\"Fat Factor\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8d89e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T00:00:06.325195Z",
     "start_time": "2022-03-18T00:00:06.306247Z"
    }
   },
   "outputs": [],
   "source": [
    "reg_fit = LinearRegression()\n",
    "reg_fit.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1702e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_pred = reg_fit.predict(X_test)\n",
    "rmse_fat = np.sqrt(mean_squared_error(y_test, fat_pred))\n",
    "\n",
    "print(np.round(rmse_fat, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642efc7",
   "metadata": {},
   "source": [
    "Podemos visualizar o modelo através de um *scatterplot* comparando previsões com valores reais no conjunto de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = fat_pred, y = y_test, s = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb050974",
   "metadata": {},
   "source": [
    "Agora, vamos testar com menos componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4080a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_t = PCA(n_components=30)\n",
    "pca_t.fit(X_sca)\n",
    "X_pca = pca_t.transform(X_sca)\n",
    "y = food[\"Fat Factor\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, random_state = 42)\n",
    "reg_fit = LinearRegression()\n",
    "reg_fit.fit(X_train, y_train)\n",
    "\n",
    "fat_pred = reg_fit.predict(X_test)\n",
    "rmse_fat = np.sqrt(mean_squared_error(y_test, fat_pred))\n",
    "\n",
    "print(np.round(rmse_fat, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = fat_pred, y = y_test, s = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b125936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207.6666717529297px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "014f4a4a5af8f0104b12c029e500f4146d6d785e8cf714d2a35b7a9514230cd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
